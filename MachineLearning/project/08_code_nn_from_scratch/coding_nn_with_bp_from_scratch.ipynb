{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coding_nn_with_bp_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9EbE8hsv75Qx8q/FolQ2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SummerLife/EmbeddedSystem/blob/master/MachineLearning/project/08_code_nn_from_scratch/coding_nn_with_bp_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0AJK-lokeNv",
        "colab_type": "text"
      },
      "source": [
        "# Coding a Neural Network with Backpropagation from scratch\n",
        "\n",
        "- Initialize Network\n",
        "- Forward Propagate\n",
        "- Back Propagate Error\n",
        "- Train Network\n",
        "- Predict\n",
        "- Seeds Dataset Case Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trbkkR6Gk4OW",
        "colab_type": "text"
      },
      "source": [
        "## 1. Initialize Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyiTEZ_RgbTu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "432eb919-b2ee-464a-a84c-232318e04f11"
      },
      "source": [
        "from random import seed\n",
        "from random import random\n",
        "\n",
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "    network = list()\n",
        "    hidden_layer = [{\"weights\": [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "    network.append(hidden_layer)\n",
        "    output_layer = [{\"weights\": [random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "    network.append(output_layer)\n",
        "    return network\n",
        "\n",
        "seed(1)\n",
        "network = initialize_network(2,1,2)\n",
        "for layer in network:\n",
        "    print(layer)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
            "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qchtDuxdnSzY",
        "colab_type": "text"
      },
      "source": [
        "## 2. Forward Propagate\n",
        "\n",
        "We can break forward propagation down into three parts:\n",
        "\n",
        "1. Neuron Activation\n",
        "2. Neuron Transfer\n",
        "3. Forward Propagation\n",
        "\n",
        "### 2.1 Neuron Activation\n",
        "\n",
        "`activation = sum(weight_i * input_i) + bias`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6d1tUSbnzZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "    activation = weights[-1]\n",
        "    for i in range(len(weights) - 1):\n",
        "        activation += weights[i] * inputs[i]\n",
        "    return activation"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miR21ajCpbi-",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Neuron Transfer\n",
        "\n",
        "Once a neuron is activated, we need to transfer the activation to see what the neuron output actually is.\n",
        "\n",
        "The sigmoid activation function looks like an S shape, it's also called the logistic function. It can take any input value and produce an number between 0 and 1 on an S-curve. It is also a function of which we can easily calculate the derivatice(slope) that we will need later when backpropagating error.\n",
        "\n",
        "We can transfer an activation function using the sigmoed function as follows:\n",
        "\n",
        "`output = 1 / (1 + e^(-activation))`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foOJ9XOrpQK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transfer neuron activation\n",
        "from math import exp\n",
        "\n",
        "def transfer(activation):\n",
        "    return 1.0 / (1.0 + exp(-activation))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gmPYrnCrxbW",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Forward Propagation\n",
        "\n",
        "Forward propagating an input is straightforward.\n",
        "\n",
        "We work through each layer of our network calculating the outputs for each neuron. All of the outputs from on layer become in puts to the neurons on the next layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEdCUDsXrKTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "    inputs = row\n",
        "    for layer in network:\n",
        "        new_inputs = []\n",
        "        for neuron in layer:\n",
        "            activation = activate(neuron['weights'], inputs)\n",
        "            neuron['output'] = transfer(activation)\n",
        "            new_inputs.append(neuron['output'])\n",
        "        inputs = new_inputs\n",
        "    return inputs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fmllGPZuovP",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Test out the forward propagation of out network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9bdgPgSufCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "debf4caa-813d-44b5-fe67-eb90d1cb5dbd"
      },
      "source": [
        "row = [1, 0, None]\n",
        "output = forward_propagate(network, row)\n",
        "print(output)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6629970129852887, 0.7253160725279748]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q100NQ2SxIt_",
        "colab_type": "text"
      },
      "source": [
        "## 3. Back Propagate Error\n",
        "\n",
        "Error is calculated between the expected outputs and the outputs forward propagated from the network. These errors are then propagated backward through the netwoek from the output layer to the hidden layer, assigning blame for the error and updating weights as they go.\n",
        "\n",
        "1. Transfer Derivative\n",
        "2. Error Backpropagation\n",
        "\n",
        "### 3.1 Transfer Derivative\n",
        "\n",
        "We are using the sigmoid transfer function, the derivative of which can be calculated as follows:\n",
        "\n",
        "```\n",
        "derivative = output * (1.0 - output)\n",
        "```\n",
        "\n",
        "Using function named `transfer_derivative()` to implements this equation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fJTj3nlvFqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "    return output * (1.0 - output)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZBM-iG38nz3",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Error Backpropagation\n",
        "\n",
        "The error for a given neuron can be calculated as follows:\n",
        "\n",
        "`error = (expected - output) * transfer_derivative(output)`\n",
        "\n",
        "\n",
        "The error signal for a neuron in the hidden layer is calculated as the weighted error of each neuron in the output layer. Think of the error traveling back along the weights of the output layer to the neurons in the hidden layer.\n",
        "\n",
        "The back-propagated error signal is accumulated and then used to determine the error for the neuron in the hidden layer, as follows:\n",
        "\n",
        "`error = (weight_k * error_j) * transfer_derivative(output)`\n",
        "\n",
        "Where error_j is the error signal from the jth neuron in the output layer, weight_k is the weight that connects the kth neuron to the current neuron and output is the output for the current neuron.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqVpngAfpuBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Backpropagate error and store in neurons\n",
        "\n",
        "def backward_propagate_error(network, expected):\n",
        "    for i in reversed(range(len(network))):\n",
        "        layer = network[i]\n",
        "        errors = list()\n",
        "\n",
        "        if i != len(network) - 1:\n",
        "            for j in range(len(layer)):\n",
        "                error = 0.0\n",
        "                for neuron in network[i + 1]:\n",
        "                    error += (neuron['weights'][j] * neuron['delta'])\n",
        "                    errors.append(error)\n",
        "        else:\n",
        "            for j in range(len(layer)):\n",
        "                neuron = layer[j]\n",
        "                errors.append(expected[j] - neuron['output'])\n",
        "\n",
        "        for j in range(len(layer)):\n",
        "            neuron = layer[j]\n",
        "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zzFy8O3qg7d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d59bdc10-8030-4418-faa3-36bd210352bc"
      },
      "source": [
        "# test backpropagation of error\n",
        "network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
        "\t\t[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "expected = [0, 1]\n",
        "\n",
        "backward_propagate_error(network, expected)\n",
        "\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': -0.007668854370284511}]\n",
            "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': 0.0771723774346327}]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}