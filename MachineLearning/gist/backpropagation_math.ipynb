{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backpropagation_math.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMO97nhzTnqVT9wktVJbH1r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SummerLife/EmbeddedSystem/blob/master/MachineLearning/gist/backpropagation_math.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUpT82UKv77p",
        "colab_type": "text"
      },
      "source": [
        "## Loss $C_{0}$\n",
        "\n",
        "the squared difference of the activation output and the desired output for node \n",
        "$j$ in the output layer $L$. This can be interpreted as the loss for node $j$ in layer $L$.\n",
        "\n",
        "$$\\left( a_{j}^{(L)}-y_{j}\\right) ^{2}$$\n",
        "\n",
        "Therefore, to calculate the total loss, we should sum this squared difference for each node $j$ in the output layer $L$.\n",
        "\n",
        "This is expressed as\n",
        "\n",
        "$$C_{0}=\\sum_{j=0}^{n-1}\\left( a_{j}^{(L)}-y_{j}\\right) ^{2}\\text{.}$$\n",
        "\n",
        "## Input $z_{j}^{(l)}$\n",
        "\n",
        "We know that the input for node $j$ in layer $l$ is the weighted sum of the activation outputs from the previous layer $lâˆ’1$.\n",
        "\n",
        "An individual term from the sum looks like this:\n",
        "\n",
        "$$w_{jk}^{(l)}a_{k}^{(l-1)}$$\n",
        "\n",
        "the input for a given node $j$ in layer $l$ is expressed as\n",
        "\n",
        "\n",
        "$$z_{j}^{(l)}=\\sum_{k=0}^{n-1}w_{jk}^{(l)}a_{k}^{(l-1)}\\text{.}$$\n",
        "\n",
        "## Activation Output \n",
        "\n",
        "the activation output of node $j$ in layer $l$ is expressed as\n",
        "\n",
        "$$a_{j}^{(l)}=g^{\\left( l\\right) }\\left( z_{j}^{\\left( l\\right) }\\right) \\text{.}$$\n",
        "\n",
        "## Expressing $C_{0}$ As A Composition Of Functions\n",
        "\n",
        "Recall the definition of $C_{0}$:\n",
        "\n",
        "$$C_{0}=\\sum_{j=0}^{n-1}\\left( a_{j}^{(L)}-y_{j}\\right) ^{2}\\text{.}$$\n",
        "\n",
        "the loss of a single node $j$ in the output layer $L$ can be expressed as\n",
        "\n",
        "$$C_{0_{j}}=\\left( a_{j}^{(L)}-y_{j}\\right) ^{2}\\text{.}$$\n",
        "\n",
        "$$C_{0_{j}}\\left( a_{j}^{\\left( L\\right) }\\right) \\text{.}$$\n",
        "\n",
        "$$a_{j}^{(L)}=g^{\\left( L\\right) }\\left( z_{j}^{\\left( L\\right) }\\right) \\text{.}$$\n",
        "\n",
        "$$z_{j}^{\\left( L\\right) }\\left( w_{j}^{\\left( L\\right) }\\right) \\text{.}$$\n",
        "\n",
        "we can see that $C_{0}$ is a composition of functions.\n",
        "\n",
        "$$C_{0_{j}}=C_{0_{j}}\\left( a_{j}^{\\left( L\\right) }\\left(\\rule[-0.1in]{0in}{0.3in}z_{j}^{\\left( L\\right) }\\left( w_{j}^{\\left(L\\right) }\\right) \\right) \\right) \\text{.}$$\n",
        "\n",
        "$$C_{0}=\\sum_{j=0}^{n-1}C_{0_{j}}\\text{}$$\n",
        "\n"
      ]
    }
  ]
}